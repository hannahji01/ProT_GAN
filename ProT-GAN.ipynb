{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4002b242",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import biopython to use the SeqIO module for reading and writing FASTA sequence files\n",
    "# !pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30f965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:04:37.561292: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 18:04:37.604487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 18:04:38.874492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Physical GPUs\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "import tensorflow as tf\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Lambda, Activation, MultiHeadAttention, Embedding, Reshape, Dropout, TimeDistributed, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Set up GPUs for training\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(len(gpus), \"Physical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e418f",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f16003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of transcription factor proteins is: 442.46913202933985\n",
      "The average length of non-transcription factor proteins is: 296.9079100387311\n"
     ]
    }
   ],
   "source": [
    "# Load your FASTA files\n",
    "transcription_factors = list(SeqIO.parse('transcription_factors.fasta', 'fasta'))\n",
    "non_transcription_factors = list(SeqIO.parse('non_transcription_factors.fasta', 'fasta'))\n",
    "\n",
    "# Calculate the average length of transcription factor proteins\n",
    "avg_len_tfs = np.mean([len(record.seq) for record in transcription_factors])\n",
    "print(f\"The average length of transcription factor proteins is: {avg_len_tfs}\")\n",
    "\n",
    "# Calculate the average length of non-transcription factor proteins\n",
    "avg_len_non_tfs = np.mean([len(record.seq) for record in non_transcription_factors])\n",
    "print(f\"The average length of non-transcription factor proteins is: {avg_len_non_tfs}\")\n",
    "\n",
    "# Assuming 'non_transcription_factors' is a list of SeqRecord objects from Biopython.\n",
    "indices = np.arange(len(non_transcription_factors))\n",
    "sampled_indices = np.random.choice(indices, size=len(transcription_factors) // 5, replace=False)\n",
    "sampled_non_tfs = [non_transcription_factors[i] for i in sampled_indices]\n",
    "\n",
    "# Load the encoding file and create a mapping from amino acids to their 3-number encoding\n",
    "def load_encoding(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip the header\n",
    "        encoding_map = {row[0]: np.array(row[1:], dtype=np.float32) for row in csv_reader}\n",
    "    return encoding_map\n",
    "\n",
    "# Create a function to encode sequences using the loaded encoding map\n",
    "def three_number_encode(sequence, encoding_map):\n",
    "    return np.array([encoding_map[char] for char in sequence])\n",
    "\n",
    "# Assuming `encoding_map` is already created from the CSV file\n",
    "# Now, let's redefine the filter_and_pad_sequences function to use the new encoding\n",
    "def filter_and_pad_sequences(sequences, encoding_map, min_length=200, max_length=300, padding_value=0):\n",
    "    # Filter sequences based on the length criteria\n",
    "    filtered_sequences = [seq for seq in sequences if min_length <= len(seq) <= max_length]\n",
    "    \n",
    "    # Convert sequences to a list of lists where each inner list is the three-number encoded sequence\n",
    "    encoded_sequences = [three_number_encode(str(seq.seq), encoding_map) for seq in filtered_sequences]\n",
    "    \n",
    "    # Pad sequences to the max_length\n",
    "    padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding='post', value=padding_value, dtype='float32')\n",
    "    \n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Usage example\n",
    "# Load the encoding map from the CSV file\n",
    "encoding_map = load_encoding('umap_3d_coordinates_scaled.csv')\n",
    "\n",
    "# Apply the function to your transcription factors and non-transcription factors\n",
    "padded_tfs = filter_and_pad_sequences(transcription_factors, encoding_map)\n",
    "padded_non_tfs = filter_and_pad_sequences(non_transcription_factors, encoding_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fa253",
   "metadata": {},
   "source": [
    "# Model Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73e2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "Number of devices: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:04:46.731004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14947 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:5e:00.0, compute capability: 7.5\n",
      "2023-12-18 18:04:46.731539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14947 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:af:00.0, compute capability: 7.5\n",
      "2023-12-18 18:04:46.732061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14947 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:d8:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 64)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 900)                  58500     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 300, 3)               0         ['dense_33[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 300, 3)               0         ['reshape[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 300, 3)               0         ['tf.math.multiply[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (M  (None, 300, 3)               93        ['tf.__operators__.add[0][0]',\n",
      " ultiHeadAttention)                                                  'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 300, 3)               0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'multi_head_attention_16[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_32 (La  (None, 300, 3)               6         ['tf.__operators__.add_1[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 300, 512)             2048      ['layer_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 300, 3)               1539      ['dense_34[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 300, 3)               0         ['layer_normalization_32[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'dense_35[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_33 (La  (None, 300, 3)               6         ['tf.__operators__.add_2[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (M  (None, 300, 3)               93        ['layer_normalization_33[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_33[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 300, 3)               0         ['layer_normalization_33[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'multi_head_attention_17[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_34 (La  (None, 300, 3)               6         ['tf.__operators__.add_3[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 300, 512)             2048      ['layer_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_37 (Dense)            (None, 300, 3)               1539      ['dense_36[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 300, 3)               0         ['layer_normalization_34[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'dense_37[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_35 (La  (None, 300, 3)               6         ['tf.__operators__.add_4[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (M  (None, 300, 3)               93        ['layer_normalization_35[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_35[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 300, 3)               0         ['layer_normalization_35[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'multi_head_attention_18[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_36 (La  (None, 300, 3)               6         ['tf.__operators__.add_5[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_38 (Dense)            (None, 300, 512)             2048      ['layer_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_39 (Dense)            (None, 300, 3)               1539      ['dense_38[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 300, 3)               0         ['layer_normalization_36[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'dense_39[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_37 (La  (None, 300, 3)               6         ['tf.__operators__.add_6[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (M  (None, 300, 3)               93        ['layer_normalization_37[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_37[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 300, 3)               0         ['layer_normalization_37[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'multi_head_attention_19[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_38 (La  (None, 300, 3)               6         ['tf.__operators__.add_7[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_40 (Dense)            (None, 300, 512)             2048      ['layer_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 300, 3)               1539      ['dense_40[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 300, 3)               0         ['layer_normalization_38[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'dense_41[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_39 (La  (None, 300, 3)               6         ['tf.__operators__.add_8[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (M  (None, 300, 3)               93        ['layer_normalization_39[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_39[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 300, 3)               0         ['layer_normalization_39[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'multi_head_attention_20[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_40 (La  (None, 300, 3)               6         ['tf.__operators__.add_9[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " dense_42 (Dense)            (None, 300, 512)             2048      ['layer_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_43 (Dense)            (None, 300, 3)               1539      ['dense_42[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 300, 3)               0         ['layer_normalization_40[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_43[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_41 (La  (None, 300, 3)               6         ['tf.__operators__.add_10[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_21 (M  (None, 300, 3)               93        ['layer_normalization_41[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_41[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 300, 3)               0         ['layer_normalization_41[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_21[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_42 (La  (None, 300, 3)               6         ['tf.__operators__.add_11[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_44 (Dense)            (None, 300, 512)             2048      ['layer_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_45 (Dense)            (None, 300, 3)               1539      ['dense_44[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (T  (None, 300, 3)               0         ['layer_normalization_42[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_45[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_43 (La  (None, 300, 3)               6         ['tf.__operators__.add_12[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (M  (None, 300, 3)               93        ['layer_normalization_43[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_43[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (T  (None, 300, 3)               0         ['layer_normalization_43[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_44 (La  (None, 300, 3)               6         ['tf.__operators__.add_13[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 300, 512)             2048      ['layer_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 300, 3)               1539      ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (T  (None, 300, 3)               0         ['layer_normalization_44[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_47[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_45 (La  (None, 300, 3)               6         ['tf.__operators__.add_14[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_23 (M  (None, 300, 3)               93        ['layer_normalization_45[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_45[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (T  (None, 300, 3)               0         ['layer_normalization_45[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_23[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_46 (La  (None, 300, 3)               6         ['tf.__operators__.add_15[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 300, 512)             2048      ['layer_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 300, 3)               1539      ['dense_48[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (T  (None, 300, 3)               0         ['layer_normalization_46[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_49[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_47 (La  (None, 300, 3)               6         ['tf.__operators__.add_16[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (M  (None, 300, 3)               93        ['layer_normalization_47[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_47[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (T  (None, 300, 3)               0         ['layer_normalization_47[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_48 (La  (None, 300, 3)               6         ['tf.__operators__.add_17[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_50 (Dense)            (None, 300, 512)             2048      ['layer_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_51 (Dense)            (None, 300, 3)               1539      ['dense_50[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (T  (None, 300, 3)               0         ['layer_normalization_48[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_51[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_49 (La  (None, 300, 3)               6         ['tf.__operators__.add_18[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_25 (M  (None, 300, 3)               93        ['layer_normalization_49[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_49[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (T  (None, 300, 3)               0         ['layer_normalization_49[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_25[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_50 (La  (None, 300, 3)               6         ['tf.__operators__.add_19[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 300, 512)             2048      ['layer_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_53 (Dense)            (None, 300, 3)               1539      ['dense_52[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (T  (None, 300, 3)               0         ['layer_normalization_50[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_53[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_51 (La  (None, 300, 3)               6         ['tf.__operators__.add_20[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_26 (M  (None, 300, 3)               93        ['layer_normalization_51[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_51[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (T  (None, 300, 3)               0         ['layer_normalization_51[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_26[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_52 (La  (None, 300, 3)               6         ['tf.__operators__.add_21[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_54 (Dense)            (None, 300, 512)             2048      ['layer_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_55 (Dense)            (None, 300, 3)               1539      ['dense_54[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (T  (None, 300, 3)               0         ['layer_normalization_52[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_55[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_53 (La  (None, 300, 3)               6         ['tf.__operators__.add_22[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_27 (M  (None, 300, 3)               93        ['layer_normalization_53[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_53[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (T  (None, 300, 3)               0         ['layer_normalization_53[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_27[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_54 (La  (None, 300, 3)               6         ['tf.__operators__.add_23[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_56 (Dense)            (None, 300, 512)             2048      ['layer_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_57 (Dense)            (None, 300, 3)               1539      ['dense_56[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (T  (None, 300, 3)               0         ['layer_normalization_54[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_57[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_55 (La  (None, 300, 3)               6         ['tf.__operators__.add_24[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_28 (M  (None, 300, 3)               93        ['layer_normalization_55[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_55[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (T  (None, 300, 3)               0         ['layer_normalization_55[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_28[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_56 (La  (None, 300, 3)               6         ['tf.__operators__.add_25[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_58 (Dense)            (None, 300, 512)             2048      ['layer_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_59 (Dense)            (None, 300, 3)               1539      ['dense_58[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (T  (None, 300, 3)               0         ['layer_normalization_56[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_59[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_57 (La  (None, 300, 3)               6         ['tf.__operators__.add_26[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_29 (M  (None, 300, 3)               93        ['layer_normalization_57[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_57[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (T  (None, 300, 3)               0         ['layer_normalization_57[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_29[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_58 (La  (None, 300, 3)               6         ['tf.__operators__.add_27[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_60 (Dense)            (None, 300, 512)             2048      ['layer_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_61 (Dense)            (None, 300, 3)               1539      ['dense_60[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (T  (None, 300, 3)               0         ['layer_normalization_58[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_61[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_59 (La  (None, 300, 3)               6         ['tf.__operators__.add_28[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_30 (M  (None, 300, 3)               93        ['layer_normalization_59[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_59[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (T  (None, 300, 3)               0         ['layer_normalization_59[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_30[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_60 (La  (None, 300, 3)               6         ['tf.__operators__.add_29[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_62 (Dense)            (None, 300, 512)             2048      ['layer_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_63 (Dense)            (None, 300, 3)               1539      ['dense_62[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (T  (None, 300, 3)               0         ['layer_normalization_60[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_63[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_61 (La  (None, 300, 3)               6         ['tf.__operators__.add_30[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_31 (M  (None, 300, 3)               93        ['layer_normalization_61[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_61[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_61[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (T  (None, 300, 3)               0         ['layer_normalization_61[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'multi_head_attention_31[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_62 (La  (None, 300, 3)               6         ['tf.__operators__.add_31[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_64 (Dense)            (None, 300, 512)             2048      ['layer_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_65 (Dense)            (None, 300, 3)               1539      ['dense_64[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (T  (None, 300, 3)               0         ['layer_normalization_62[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_65[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_63 (La  (None, 300, 3)               6         ['tf.__operators__.add_32[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_66 (Dense)            (None, 300, 3)               12        ['layer_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 117584 (459.31 KB)\n",
      "Trainable params: 117584 (459.31 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 300, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (T  (None, 300, 3)               0         ['input_3[0][0]']             \n",
      " FOpLambda)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (M  (None, 300, 3)               93        ['tf.__operators__.add_33[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'tf.__operators__.add_33[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'tf.__operators__.add_33[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_32[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (T  (None, 300, 3)               0         ['tf.__operators__.add_33[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dropout_32[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_64 (La  (None, 300, 3)               6         ['tf.__operators__.add_34[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_67 (Dense)            (None, 300, 512)             2048      ['layer_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_68 (Dense)            (None, 300, 3)               1539      ['dense_67[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (T  (None, 300, 3)               0         ['layer_normalization_64[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_68[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_65 (La  (None, 300, 3)               6         ['tf.__operators__.add_35[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (M  (None, 300, 3)               93        ['layer_normalization_65[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_65[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_33[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (T  (None, 300, 3)               0         ['layer_normalization_65[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_33[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_66 (La  (None, 300, 3)               6         ['tf.__operators__.add_36[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_69 (Dense)            (None, 300, 512)             2048      ['layer_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_70 (Dense)            (None, 300, 3)               1539      ['dense_69[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (T  (None, 300, 3)               0         ['layer_normalization_66[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_70[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_67 (La  (None, 300, 3)               6         ['tf.__operators__.add_37[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (M  (None, 300, 3)               93        ['layer_normalization_67[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_67[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_34[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (T  (None, 300, 3)               0         ['layer_normalization_67[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_34[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_68 (La  (None, 300, 3)               6         ['tf.__operators__.add_38[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 300, 512)             2048      ['layer_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_72 (Dense)            (None, 300, 3)               1539      ['dense_71[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (T  (None, 300, 3)               0         ['layer_normalization_68[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_72[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_69 (La  (None, 300, 3)               6         ['tf.__operators__.add_39[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (M  (None, 300, 3)               93        ['layer_normalization_69[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_69[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_35[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_40 (T  (None, 300, 3)               0         ['layer_normalization_69[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_35[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_70 (La  (None, 300, 3)               6         ['tf.__operators__.add_40[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_73 (Dense)            (None, 300, 512)             2048      ['layer_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_74 (Dense)            (None, 300, 3)               1539      ['dense_73[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_41 (T  (None, 300, 3)               0         ['layer_normalization_70[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_74[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_71 (La  (None, 300, 3)               6         ['tf.__operators__.add_41[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_36 (M  (None, 300, 3)               93        ['layer_normalization_71[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_71[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_36[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_42 (T  (None, 300, 3)               0         ['layer_normalization_71[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_72 (La  (None, 300, 3)               6         ['tf.__operators__.add_42[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_75 (Dense)            (None, 300, 512)             2048      ['layer_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_76 (Dense)            (None, 300, 3)               1539      ['dense_75[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_43 (T  (None, 300, 3)               0         ['layer_normalization_72[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_76[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_73 (La  (None, 300, 3)               6         ['tf.__operators__.add_43[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (M  (None, 300, 3)               93        ['layer_normalization_73[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_73[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_37[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_44 (T  (None, 300, 3)               0         ['layer_normalization_73[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_74 (La  (None, 300, 3)               6         ['tf.__operators__.add_44[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_77 (Dense)            (None, 300, 512)             2048      ['layer_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_78 (Dense)            (None, 300, 3)               1539      ['dense_77[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_45 (T  (None, 300, 3)               0         ['layer_normalization_74[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_78[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_75 (La  (None, 300, 3)               6         ['tf.__operators__.add_45[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (M  (None, 300, 3)               93        ['layer_normalization_75[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_75[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_38[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_46 (T  (None, 300, 3)               0         ['layer_normalization_75[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_38[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_76 (La  (None, 300, 3)               6         ['tf.__operators__.add_46[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_79 (Dense)            (None, 300, 512)             2048      ['layer_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_80 (Dense)            (None, 300, 3)               1539      ['dense_79[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_47 (T  (None, 300, 3)               0         ['layer_normalization_76[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_80[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_77 (La  (None, 300, 3)               6         ['tf.__operators__.add_47[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (M  (None, 300, 3)               93        ['layer_normalization_77[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_77[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization_77[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)        (None, 300, 3)               0         ['multi_head_attention_39[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_48 (T  (None, 300, 3)               0         ['layer_normalization_77[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dropout_39[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_78 (La  (None, 300, 3)               6         ['tf.__operators__.add_48[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_81 (Dense)            (None, 300, 512)             2048      ['layer_normalization_78[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_82 (Dense)            (None, 300, 3)               1539      ['dense_81[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_49 (T  (None, 300, 3)               0         ['layer_normalization_78[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'dense_82[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_79 (La  (None, 300, 3)               6         ['tf.__operators__.add_49[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 3)                    0         ['layer_normalization_79[0][0]\n",
      " alMaxPooling1D)                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_83 (Dense)            (None, 1)                    4         ['global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29540 (115.39 KB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 29540 (115.39 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "save_interval = 10\n",
    "sequence_length = 300  # Adjust based on your sequences\n",
    "latent_dim = 64  # Latent dimension for the generator\n",
    "num_heads = 4\n",
    "embed_dim = 256\n",
    "num_layers = 16\n",
    "num_layers_g = 8\n",
    "num_layers_d = 8\n",
    "ff_dim = 512\n",
    "dropout_rate=0.3\n",
    "encoding_size = 3  # Each amino acid is encoded by 3 numbers\n",
    "\n",
    "# Define the transformer encoder architecture with positional encoding\n",
    "def build_transformer_encoder(seq_len, num_heads, embed_dim, num_layers, ff_dim, encoding_size, dropout_rate=dropout_rate):\n",
    "    inputs = layers.Input(shape=(seq_len, encoding_size))\n",
    "\n",
    "    # Add positional encoding as a layer\n",
    "    positional_encoding = get_positional_encoding(seq_len, encoding_size)\n",
    "    # Convert positional_encoding to a tensor to add it to the inputs\n",
    "    pos_enc_tensor = tf.cast(positional_encoding, dtype=tf.float32)\n",
    "    pos_enc_layer = layers.Lambda(lambda x: x + pos_enc_tensor)(inputs)\n",
    "\n",
    "    x = pos_enc_layer\n",
    "    for i in range(num_layers):\n",
    "        # Multi-Head Attention layer\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "        # Add & Norm\n",
    "        x = layers.Add()([x, attention_output])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ff_output = layers.Dense(ff_dim, activation=\"LeakyReLU\")(x)\n",
    "        ff_output = layers.Dropout(dropout_rate)(ff_output)\n",
    "        ff_output = layers.Dense(encoding_size)(ff_output)  # Match encoding_size\n",
    "        # Add & Norm\n",
    "        x = layers.Add()([x, ff_output])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    outputs = layers.Dense(encoding_size, activation='linear')(x)  # Keep encoding_size for output\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Function to generate positional encoding\n",
    "def get_positional_encoding(seq_len, d_emb):\n",
    "    pos_enc = np.zeros((seq_len, d_emb))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(d_emb):\n",
    "            if i % 2 == 0:\n",
    "                pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_emb)))\n",
    "            else:\n",
    "                pos_enc[pos, i] = np.cos(pos / (10000 ** ((2 * (i - 1)) / d_emb)))\n",
    "    return pos_enc\n",
    "\n",
    "# Define the generator network architecture\n",
    "def build_generator(latent_dim, seq_len, encoding_size, num_heads=2, num_layers=num_layers_g, dff=ff_dim):\n",
    "    # The input to the generator will be a batch of random vectors\n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "\n",
    "    # Foundation for the sequence - a dense layer with activation\n",
    "    x = Dense(units=seq_len * encoding_size, activation='LeakyReLU')(inputs)\n",
    "    x = Reshape((seq_len, encoding_size))(x)\n",
    "\n",
    "    # Add positional encoding to the input sequence\n",
    "    positional_encoding = get_positional_encoding(seq_len, encoding_size)\n",
    "    x *= tf.math.sqrt(tf.cast(encoding_size, tf.float32))\n",
    "    x += positional_encoding\n",
    "\n",
    "    # We will now build num_layers of transformer blocks\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-head attention\n",
    "        attn_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=encoding_size, dropout=0.3)(x, x, x)\n",
    "\n",
    "        # Skip connection and layer normalization\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = Dense(dff, activation='LeakyReLU')(x)\n",
    "        ffn_output = Dense(encoding_size)(ffn_output)\n",
    "\n",
    "        # Skip connection and layer normalization\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "    # Final dense layer\n",
    "    outputs = Dense(encoding_size, activation='linear')(x)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define the discriminator network architecture\n",
    "def build_discriminator(seq_len, encoding_size, num_heads=2, num_layers=num_layers_d, dff=ff_dim):\n",
    "    inputs = Input(shape=(seq_len, encoding_size))\n",
    "\n",
    "    # Add positional encoding to the input sequence\n",
    "    positional_encoding = get_positional_encoding(seq_len, encoding_size)\n",
    "    x = inputs + positional_encoding\n",
    "\n",
    "    # We will now build num_layers of transformer blocks\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-head attention\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=encoding_size)(x, x, x)\n",
    "        attn_output = Dropout(0.3)(attn_output)  # Optional; add dropout for regularization\n",
    "        # Skip connection and layer normalization\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        # Feed-forward network\n",
    "        ffn_output = Dense(dff, activation='LeakyReLU')(x)\n",
    "        ffn_output = Dense(encoding_size)(ffn_output)\n",
    "        # Skip connection and layer normalization\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "    # After the transformer blocks, we apply global average pooling to convert the sequence to a single vector\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    # Final dense layer for binary classification\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Assuming `padded_tfs` and `padded_non_tfs` are your padded sequences using the new 3-number encoding\n",
    "# Concatenate the sequences and create labels\n",
    "X = np.concatenate((padded_tfs, padded_non_tfs), axis=0)\n",
    "y = np.concatenate((np.ones(len(padded_tfs)), np.zeros(len(padded_non_tfs))), axis=0)\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # Build the transformer encoder\n",
    "    transformer_encoder = build_transformer_encoder(\n",
    "        sequence_length, num_heads, embed_dim, num_layers, ff_dim, encoding_size)\n",
    "\n",
    "    # Build the generator\n",
    "    generator = build_generator(latent_dim, sequence_length, encoding_size)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator(sequence_length, encoding_size)\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Noise input for the generator\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    generated_sequence = generator(noise)\n",
    "\n",
    "    # Make the discriminator non-trainable when we are training the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated sequences as input and determines validity\n",
    "    validity = discriminator(generated_sequence)\n",
    "\n",
    "    # The combined model (stacked generator and discriminator)\n",
    "    combined = Model(noise, validity)\n",
    "    combined.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Print model summaries\n",
    "print(generator.summary())\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6480e0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5210b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 148ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:05:01.632476: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020TensorDataset:96\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 130 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Collective all_reduce tensors: 130 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:05:29.650290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1467344865e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-18 18:05:29.650337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "2023-12-18 18:05:29.650346: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5\n",
      "2023-12-18 18:05:29.650351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Quadro RTX 5000, Compute Capability 7.5\n",
      "2023-12-18 18:05:29.659734: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-18 18:05:29.674759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-12-18 18:05:29.677023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-12-18 18:05:29.679386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-12-18 18:05:29.771350: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:05:39.292715: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:115\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 130 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:05:53.869828: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:134\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 64\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 260 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 260 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "0 [D loss: 1.0779261589050293, acc.: 0.00%] [G loss: 0.42105424404144287]\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:07:25.274363: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:249\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:25.504064: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:268\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:25.692367: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:287\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 64\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.065669059753418, acc.: 0.00%] [G loss: 0.4271779954433441]\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:07:26.546978: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:402\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:26.722808: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:421\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:26.885883: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:440\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 64\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [D loss: 1.0498957633972168, acc.: 0.00%] [G loss: 0.41578441858291626]\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:07:27.639877: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:555\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:27.815445: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:574\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 300\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-12-18 18:07:27.984167: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_2\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:593\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 64\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 16\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [D loss: 1.0269793272018433, acc.: 0.00%] [G loss: 0.43960627913475037]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def plot_training_loss(discriminator_loss, generator_loss, save_interval):\n",
    "    epochs = range(len(discriminator_loss))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, [loss[0] for loss in discriminator_loss], label='Discriminator Loss')\n",
    "    plt.plot(epochs, generator_loss, label='Generator Loss')\n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "discriminator_losses = []\n",
    "generator_losses = []\n",
    "\n",
    "def train_gan(generator, discriminator, combined, transformer_encoder, X_train, y_train, epochs, batch_size, latent_dim, save_interval):\n",
    "    for epoch in range(epochs):\n",
    "        # Generate a batch of new sequences\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_seqs = generator.predict(noise)\n",
    "\n",
    "        # Encode the real and generated sequences using the transformer encoder\n",
    "        encoded_real_seqs = transformer_encoder.predict(X_train[:batch_size])\n",
    "        encoded_gen_seqs = transformer_encoder.predict(gen_seqs)\n",
    "\n",
    "        # Train the discriminator on the real and generated sequences\n",
    "        d_loss_real = discriminator.train_on_batch(encoded_real_seqs, y_train[:batch_size])\n",
    "        d_loss_fake = discriminator.train_on_batch(encoded_gen_seqs, np.zeros((batch_size, 1)))\n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_y = np.ones((batch_size, 1))\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "        discriminator_losses.append(d_loss)\n",
    "        generator_losses.append(g_loss)\n",
    "\n",
    "        # Plot the progress\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "# Call the train_gan function with the appropriate arguments\n",
    "train_gan(generator, discriminator, combined, transformer_encoder, X_train, y_train, epochs, batch_size, \n",
    "          latent_dim, save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60a9016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21188, 300, 3)\n",
      "Transformer encoder input shape: (None, 300, 3)\n",
      "Generator output shape: (None, 300, 3)\n",
      "Discriminator input shape: (None, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, sequence_length, encoding_size)\n",
    "print(\"Transformer encoder input shape:\", transformer_encoder.input_shape)  # Should match X_train's shape\n",
    "print(\"Generator output shape:\", generator.output_shape)  # Should be (None, sequence_length, encoding_size)\n",
    "print(\"Discriminator input shape:\", discriminator.input_shape)  # Should match Generator output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdabcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After training, plot the losses\n",
    "plot_training_loss(discriminator_losses, generator_losses, save_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03d097",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Hyperparameters\n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "save_interval = 10\n",
    "sequence_length = 300  # Adjust based on your sequences\n",
    "latent_dim = 64  # Latent dimension for the generator\n",
    "num_heads = 4\n",
    "embed_dim = 256\n",
    "num_layers = 16\n",
    "num_layers_g = 16\n",
    "num_layers_d = 8\n",
    "ff_dim = 512\n",
    "dropout_rate=0.3\n",
    "encoding_size = 3  # Each amino acid is encoded by 3 numbers\n",
    "with strategy.scope():\n",
    "    # Build the transformer encoder\n",
    "    transformer_encoder = build_transformer_encoder(\n",
    "        sequence_length, num_heads, embed_dim, num_layers, ff_dim, encoding_size)\n",
    "\n",
    "    # Build the generator\n",
    "    generator = build_generator(latent_dim, sequence_length, encoding_size)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator(sequence_length, encoding_size)\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Noise input for the generator\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    generated_sequence = generator(noise)\n",
    "\n",
    "    # Make the discriminator non-trainable when we are training the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated sequences as input and determines validity\n",
    "    validity = discriminator(generated_sequence)\n",
    "\n",
    "    # The combined model (stacked generator and discriminator)\n",
    "    combined = Model(noise, validity)\n",
    "    combined.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "# Load weights for the transformer_encoder model\n",
    "with open('transformer_encoder_weights.pkl', 'rb') as file:\n",
    "    transformer_encoder_weights = pickle.load(file)\n",
    "transformer_encoder.set_weights(transformer_encoder_weights)\n",
    "\n",
    "# Load weights for the generator model\n",
    "with open('generator_weights.pkl', 'rb') as file:\n",
    "    generator_weights = pickle.load(file)\n",
    "generator.set_weights(generator_weights)\n",
    "\n",
    "# Load weights for the discriminator model\n",
    "with open('discriminator_weights.pkl', 'rb') as file:\n",
    "    discriminator_weights = pickle.load(file)\n",
    "discriminator.set_weights(discriminator_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93547fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KDTree from the encoding map\n",
    "def create_kdtree(encoding_map):\n",
    "    reverse_encoding = {tuple(value): key for key, value in encoding_map.items()}\n",
    "    encoding_values = np.array(list(reverse_encoding.keys()))\n",
    "    kdtree = KDTree(encoding_values)\n",
    "    return kdtree, reverse_encoding, encoding_values\n",
    "\n",
    "# Update the nearest_amino_acid function to accept encoding_values\n",
    "def nearest_amino_acid(encoded_vector, kdtree, reverse_encoding, encoding_values):\n",
    "    dist, ind = kdtree.query([encoded_vector], k=1)\n",
    "    nearest_vector = encoding_values[ind[0][0]]\n",
    "    return reverse_encoding[tuple(nearest_vector)]\n",
    "\n",
    "# When calling generate_sequences, pass encoding_values as well\n",
    "def generate_sequences(generator, num_sequences, latent_dim, kdtree, reverse_encoding, encoding_values):\n",
    "    # Generate a batch of noise vectors\n",
    "    random_latent_vectors = np.random.normal(0, 1, size=(num_sequences, latent_dim))\n",
    "\n",
    "    # Use the trained generator to generate new sequences\n",
    "    generated_sequences = generator.predict(random_latent_vectors)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    predicted_sequences = []\n",
    "    for sequence in generated_sequences:\n",
    "        predicted_seq = ''\n",
    "        for encoded_vector in sequence:\n",
    "            # Pass encoding_values to nearest_amino_acid function\n",
    "            predicted_seq += nearest_amino_acid(encoded_vector, kdtree, reverse_encoding, encoding_values)\n",
    "        predicted_sequences.append(predicted_seq)\n",
    "\n",
    "    return predicted_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 10000\n",
    "# Create the KDTree and reverse encoding map\n",
    "kdtree, reverse_encoding, encoding_values = create_kdtree(encoding_map)\n",
    "\n",
    "predicted_sequences = generate_sequences(generator, num_sequences, latent_dim, kdtree, reverse_encoding, encoding_values)\n",
    "output_file_path = \"generated_sequences.txt\"\n",
    "\n",
    "# Print the generated sequences to output txt file to upload to UniProt BLAST\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for i, seq in enumerate(predicted_sequences):\n",
    "        header = f\">Generated_sequence_{i+1}\\n\"\n",
    "        output_file.write(header)\n",
    "        output_file.write(seq)\n",
    "        output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea26ca95-5c3a-451f-920e-5cc110009ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "Generated sequence 1: AUWMAGAUFLGGAAMLRAAAFWWGAAAYFSGAAWFMKAFAFMGAGAMWRSAYFWMGAAFMGNGGMFWRGAAAWGNALUFMFAAYWWAGANYMAAARUWAGGUFWRFAAYFFAAAUFMMAAUWWRKGAFLMMAALMMGRAALMGGAAWWMAAAYWMGAAUFWRGAAYWWGGAYWMWAUUWFRAALMYRAUGYWMAAAAWWAGAAWMRGAUUWRAGAUWNGAAUFMGAAAFWMFAUYWMKAAYWWWAAAWMRUAFLWRAAAFWMGAAUFWGAAAYWRAAAFFWAAAMMWGAALLMKGGALWR\n",
      "Generated sequence 2: AUFWNAAUFFRGGAAFMAAAUMRNAAAFWGAAAFMAGAAULUGAUAUWRAAUWWMGAALFWGAAAFARAALFMAAAAAMMGAAUYMGAALFMKAAYFFMGAAFFNAAAYFWAAAAMWGAAAFMRAAAFYWRAAUWRRAAAFRRGAALFWAAAAFSGAGAFWAGAAAWMGALAYRGAALWRGAAYWWMGAAMFAGAGUWMKAGAFWKAAAWWWGAAUWMGAUALWRGAUFWFGAGYWMRAALWMGAAAWWGGAUYMMGGAUWRGAAUFMMAAAYWRAAAUWRGAAFFMGAAUFWRGAUYYR\n",
      "Generated sequence 3: AAFWGAAUFWWRAAYYMAAAFWFGAAUFMRAAAWFMGAALFRGAUAWGRAAUWWRNGAYWKGAAFWWGGAAYRGAAAUWFGAAUFGGAAUWFGAAUYMGAARLFWFAALWARAAUMMNAAAWWRGAAAWMRAAUWWMAAUYWGGAAFWYAAAUFRGAALFWGGAAWFRGAAUYMGAUAWMNGAYUWNGAAYFRGGNYFMNGUARWAGAALWFAAAWFRGAAUFMKAAUWFGRAAFFMAAALFWMAAYWWKAAAYMFKAUFWRGAAUYFMGAAMWMAAUWMWGAALMNGAAFWWNNAUFMW\n",
      "Generated sequence 4: ASFFSAAAFMRGAALMMAAAYYMGAAAFUMGAAYWMAAAYWRAAAFFFGGAALMRGAAUWWGAUUMWGAAAFWRGAAWWFGAAAWFGAAMYWAAALUMWGAAMWFGGAWFRGAARLWMGAAUFWGAAUWMGAAUFMGAAAWWRGAAWMKGAAAFMGAAAWKGAAAMWRGAAUFMGAAAFWAGARMWWAGGLFFGGALRMGAAWRWGAAAAMRAAAUFRGAGUWMGAAAYWKAAGYWMGAAAYWRGAFWWSGAALWRGGAWFYFAGAFWNGALYWNGAAUWGGAAFMMAGAULWRGAAFNA\n",
      "Generated sequence 5: AAFWGGAAUWRAAAMRGGAAUWUGAAAFMMAAGLFWMAGYWFKAAUFMAAAUWWMGGAYWMKAAUYMAAGUFMAAAAUWMAAUWWMAAAAFWGAALWWRGAAAWMGAAUMMNAAFFWSAAAWRGAAAAWAGAAUWRGGAUFFRAMAFFSGAAWFWGGAUWWKGAKFFMGAAYWMNAAYUFGSAAWFRAAAMWRGGUALMNAAMLWAGAUFWANAAYWYAAGULWWGAAFWRNAUMWAGAALUMGAAUYWRAAAUFGGAAMUMGAAAWWNAALFWRGAAYRYGAAUFMGGAWWWMAAGYRM\n",
      "Generated sequence 6: ALFWRGAAFRWGGAUFMRAAUFMMAAUFWRAAAUWGGAAWWMFRAAWMRGAUFYMAAAAFNGAAUWWRAAAFMWGALUWAGUUYWWSAWAFMAAAUMFFGAAMFMAAAUWWAAAAFWRAAALFRSAAYWMNAAAWWWAAAFARWAAUWWAAAALRGAAAFURAALGMWGAWAWMGAAUYMGAAAWRMGARLWRRAAYWRAAAAYMSGAUYUMGGAFWMKAUFAWFGAUUWAGGAYWMAGAYFMRAAAMUMGAAFWRGAAYLRAAAYFWUGAYFFAAAAUWMGAAWWKRAALFMRGGAWFA\n",
      "Generated sequence 7: WAWMNAAAFWRRAAYFGGAAWWWGAAAYWMMAAAWRNGAWFGNAAAFMNAAAWWWGGAFWSGAAUWMRAUFFWGAAAFUNGAAAUMGAUAFMWAGAYMRGAUAYRNAAUWWNGAAFWKAAAWWMGAALFWGGAAWWGAGFYWMGAALFWGAAUFMGGAAWWRGARFMRRAAAFMGAAUFWNAARLWGGAAFMRAAUAFRRGAAWWNGAAYFWMAAFRRGAAUFMRAAAYWAGAUMFKGAAYFWNAGAFMMAGAUWRAAAUAWGAAAYMGGAAUWWAAYWFGAAAFWMGGGSMWMAAALAR\n",
      "Generated sequence 8: AAAWNAAAUFMGAYFMWKAAFYMNAGRFWMGAAUMRAAYYWRRAALFWNAAAWWRAAUFFMRGAWWWRAALWFRGAAFYRGAAFRWNAAAAWRAAAWWFAAALWRGANAWRMGAWYMWNAAAWRGAAYFMGAAYAWKAAFYFRGAAFARGAAUMWGAAUYWKAAAWAKGGALFWGAAUFWGAGAFMRGAAUMWGAAUFMGAAAFRAGGGAFRAAAUWARAAFFMGAAMWNKAAAAWRRALYWNRGUFUWRYAUAYGAAAMWFGGAAYWKAAUAWGAAAYWGWAAFWWAAAFFWAAAAYWR\n",
      "Generated sequence 9: AUWWMAAAFWRNAAYYNRAAYYWGGAAUFGAAAMFMAAAYFRGAAAUWNGALFFRGAAYWAGAALMMMGAAUWAGAYUWGMAAFWGRAAMWGSALAYWMAAALRNAAAUAWGAAUWWAAAAUWMRAAWWMKAAYMMGAAAFWGGAFFWMAAAUFMRAALLWMGAAUMWGAAULWNGAUWMRAALYWRAAUYWNGAUUFMGAAUMMRAAAWFMAAAYMMAAAUWFMAAUFFMGGGLWMGAAAFWGUAUWMKAAAUWNGAAYFMKAAAUWGGAAMWMAAAYAMGAAFWRAMUUFMNAGAUWW\n",
      "Generated sequence 10: AUFMKAALWMAGAAAFRAAAAWWGGAAFWNGAUFFRGAALFMAUAAWWKLAYFWMAAAFWRGAAAFWGGAAUMGGUYUWGMAAFYRGGAWWMRLUAFFGGAYUWRWAAWFFAAAAWWRAAAWFGNAAFWRMAGAWWAAAAFWGGGUYFWGAAAFWGAAAFWNGAALWMAAAWWMGAAUMWWAAAWWRSAULWMWAAFFYSGAUFWRAAUFWGGAAAWMWAAFUWKGAAAFWRGLLWWAGAFYWRAAUFWRAAUFWRGAALMRGAAUWMAAAUFWWAAUWWMGAAFFRSGAUWMGGGUFWW\n"
     ]
    }
   ],
   "source": [
    "# Demo 10 example sequences\n",
    "num_sequences = 10\n",
    "\n",
    "\n",
    "# Create the KDTree and reverse encoding map\n",
    "kdtree, reverse_encoding, encoding_values = create_kdtree(encoding_map)\n",
    "\n",
    "predicted_sequences = generate_sequences(generator, num_sequences, latent_dim, kdtree, reverse_encoding, encoding_values)\n",
    "\n",
    "# Print the generated sequences\n",
    "for i, seq in enumerate(predicted_sequences):\n",
    "    print(f\"Generated sequence {i+1}: {seq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
